<p align="center">
  <img src="https://img.shields.io/badge/Go-1.24-00ADD8?style=for-the-badge&logo=go&logoColor=white" alt="Go Version"/>
  <img src="https://img.shields.io/badge/Elasticsearch-8.x-005571?style=for-the-badge&logo=elasticsearch&logoColor=white" alt="Elasticsearch"/>
  <img src="https://img.shields.io/badge/PostgreSQL-15+-4169E1?style=for-the-badge&logo=postgresql&logoColor=white" alt="PostgreSQL"/>
  <img src="https://img.shields.io/badge/AWS_S3-232F3E?style=for-the-badge&logo=amazons3&logoColor=white" alt="AWS S3"/>
  <img src="https://img.shields.io/badge/Docker-Ready-2496ED?style=for-the-badge&logo=docker&logoColor=white" alt="Docker"/>
</p>

<h1 align="center">ğŸš€ Connectra</h1>
<h3 align="center">High-Performance B2B Data Platform with Hybrid Search Architecture</h3>

<p align="center">
  <strong>A production-grade backend system demonstrating advanced Go concurrency patterns, custom query language design, distributed systems principles, and enterprise-level architecture for processing millions of records with sub-second latency.</strong>
</p>

<p align="center">
  <a href="#-architectural-philosophy">Architecture</a> â€¢
  <a href="#-core-engineering-achievements">Achievements</a> â€¢
  <a href="#-advanced-concurrency-implementation">Concurrency</a> â€¢
  <a href="#-custom-query-language-vql---domain-specific-language-design">VQL</a> â€¢
  <a href="#-system-design-decisions">Design Decisions</a> â€¢
  <a href="#-api-reference">API</a>
</p>

---

## ğŸ“‹ Table of Contents

- [Architectural Philosophy](#-architectural-philosophy)
- [Core Engineering Achievements](#-core-engineering-achievements)
- [Advanced Concurrency Implementation](#-advanced-concurrency-implementation)
- [Custom Query Language (VQL) - Domain-Specific Language Design](#-custom-query-language-vql---domain-specific-language-design)
- [Hybrid Database Architecture - The CQRS-Inspired Pattern](#-hybrid-database-architecture---the-cqrs-inspired-pattern)
- [Distributed Job Processing Engine](#-distributed-job-processing-engine)
- [Security & Reliability Patterns](#-security--reliability-patterns)
- [Performance Engineering](#-performance-engineering)
- [System Design Decisions & Trade-offs](#-system-design-decisions--trade-offs)
- [Design Patterns & SOLID Principles](#-design-patterns--solid-principles)
- [Scalability Considerations](#-scalability-considerations)
- [Error Handling & Resilience](#-error-handling--resilience)
- [API Reference](#-api-reference)
- [Project Structure](#-project-structure)
- [Getting Started](#-getting-started)

---

## ğŸ— Architectural Philosophy

### High-Level System Design

```
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚   Client    â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API GATEWAY     Gin â†’ CORS â†’ Rate Limiter â†’ API Key Auth â†’ Gzip Compression     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APPLICATION     Controllers â†’ Services â†’ Repositories                           â”‚
â”‚                                    â”‚                                            â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚                        â–¼                       â–¼                                â”‚
â”‚                  VQL Engine            Parallel Fetcher                         â”‚
â”‚               (DSL â†’ ES Query)       (Goroutines + WaitGroup)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA LAYER                                                                      â”‚
â”‚                                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚    PostgreSQL     â”‚   â”‚   Elasticsearch   â”‚   â”‚      AWS S3       â”‚         â”‚
â”‚   â”‚   (Bun ORM)       â”‚   â”‚   (Full-text)     â”‚   â”‚   (Presigned)     â”‚         â”‚
â”‚   â”‚   â€¢ ACID          â”‚   â”‚   â€¢ Fuzzy Match   â”‚   â”‚   â€¢ Streaming     â”‚         â”‚
â”‚   â”‚   â€¢ UPSERT        â”‚   â”‚   â€¢ N-gram        â”‚   â”‚   â€¢ Direct Upload â”‚         â”‚
â”‚   â”‚   â€¢ Pool: 40/20   â”‚   â”‚   â€¢ Bulk API      â”‚   â”‚                   â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JOB ENGINE      Ticker â†’ Channel(1000) â†’ Worker Pool(N) â†’ Batch Upsert          â”‚
â”‚                                                                                 â”‚
â”‚   â€¢ Graceful Shutdown (SIGTERM)        â€¢ State Machine (OPENâ†’QUEUEâ†’DONE)        â”‚
â”‚   â€¢ Memory-efficient Streaming         â€¢ Thread-safe Error Aggregation          â”‚
â”‚   â€¢ Automatic Retry with Count         â€¢ Persistent Queue (PostgreSQL)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack with Engineering Justification

| Component | Technology | Why This Choice | Alternative Considered |
|-----------|------------|-----------------|----------------------|
| **Language** | Go 1.24 | Native concurrency (goroutines), zero-cost abstractions, fast compilation, single binary deployment | Rust (steeper learning curve), Java (JVM overhead) |
| **Web Framework** | Gin | Fastest Go HTTP framework (40k req/s benchmark), extensive middleware ecosystem, zero allocation router | Echo, Fiber (less mature ecosystem) |
| **Primary DB** | PostgreSQL + Bun ORM | ACID compliance for source of truth, type-safe queries, excellent connection pooling, UPSERT support | GORM (reflection-heavy), raw SQL (error-prone) |
| **Search Engine** | Elasticsearch 8.x | Distributed full-text search, fuzzy matching, aggregations, horizontal scaling | Meilisearch (less mature), Algolia (costly) |
| **Object Storage** | AWS S3 SDK v2 | Industry standard, presigned URLs for direct uploads, streaming support | MinIO (self-hosted alternative) |
| **Configuration** | Viper | Multi-source config (env, files), hot reload, structured config with reflection | godotenv (limited features) |
| **CLI Framework** | Cobra | Subcommand architecture for different run modes (server, jobs), flag parsing | urfave/cli (less powerful) |
| **Logging** | Zerolog | Zero-allocation JSON logging, structured output, log levels | Zap (similar), logrus (slower) |

---

## âš¡ Core Engineering Achievements

### 1. Concurrent Multi-Store Writes with Thread-Safe Error Aggregation

**Problem:** Insert data into 5 different stores (2 PostgreSQL tables, 2 Elasticsearch indices, 1 filters table) efficiently.

**Solution:** Parallel goroutines with mutex-protected error collection:

```go
// batchInsertService.go - Demonstrating concurrent writes to multiple stores
func (s *batchUpsertService) UpsertBatch(pgCompanies []*models.PgCompany, pgContacts []*models.PgContact,
    esCompanies []*models.ElasticCompany, esContacts []*models.ElasticContact) error {

    var wg sync.WaitGroup
    var mu sync.Mutex
    var insertionError error
    wg.Add(2) // Companies and Contacts in parallel

    // Goroutine 1: Company data (PostgreSQL + Elasticsearch in parallel)
    go func() {
        defer wg.Done()
        if err := s.companyService.BulkUpsert(pgCompanies, esCompanies); err != nil {
            mu.Lock()
            insertionError = err
            mu.Unlock()
        }
    }()

    // Goroutine 2: Contact data (PostgreSQL + Elasticsearch in parallel)
    go func() {
        defer wg.Done()
        if err := s.contactService.BulkUpsert(pgContacts, esContacts); err != nil {
            mu.Lock()
            insertionError = err
            mu.Unlock()
        }
    }()

    wg.Wait() // Wait for all operations to complete
    return insertionError
}
```

**Inside each service (nested parallelism):**
```go
// contactService.go - 3 parallel writes within a single service call
func (s *ContactService) BulkUpsertToDb(pgContacts []*models.PgContact,
    esContacts []*models.ElasticContact, filtersData []*models.ModelFilterData) error {

    var wg sync.WaitGroup
    var mu sync.Mutex
    var insertionError error
    wg.Add(3) // PostgreSQL, Elasticsearch, and FiltersData in parallel

    go func() {
        defer wg.Done()
        if _, err := s.contactPgRepository.BulkUpsert(pgContacts); err != nil {
            mu.Lock()
            insertionError = err
            mu.Unlock()
        }
    }()

    go func() {
        defer wg.Done()
        if _, err := s.contactElasticRepository.BulkUpsert(esContacts); err != nil {
            mu.Lock()
            insertionError = err
            mu.Unlock()
        }
    }()

    go func() {
        defer wg.Done()
        if err := s.filtersDataRepository.BulkUpsert(filtersData); err != nil {
            mu.Lock()
            insertionError = err
            mu.Unlock()
        }
    }()

    wg.Wait()
    return insertionError
}
```

**Engineering Significance:**
- **5x faster** than sequential writes (parallel I/O)
- **Thread-safe** error aggregation prevents race conditions
- **WaitGroup pattern** ensures all operations complete before returning
- **Nested parallelism** for maximum throughput

---

### 2. Channel-Based Worker Pool with Backpressure Management

**Problem:** Process jobs from database without overwhelming the system or losing jobs on shutdown.

**Solution:** Producer-consumer pattern with buffered channels and graceful shutdown:

```go
// s3_file_insertions.go - Production-grade job processing
func InsertFileJob(ctx context.Context) {
    insertJob := NewInsertJob()
    var wg sync.WaitGroup
    jobsChannel := make(chan models.ModelJobs, 1000) // Buffered for backpressure

    ticker := time.NewTicker(time.Duration(conf.JobConfig.TickerInterval) * time.Second)
    defer func() {
        ticker.Stop()
        wg.Wait()
        log.Info().Msg("All workers stopped")
    }()

    // Spawn configurable number of workers
    for i := 0; i < conf.JobConfig.ParallelJobs; i++ {
        wg.Add(1)
        go insertJob.Run(&wg, 0, jobsChannel) // Each worker listens on same channel
    }

    inQueSize := conf.JobConfig.JobInQueuedSize
    for {
        select {
        case <-ctx.Done(): // Graceful shutdown on SIGTERM/SIGINT
            log.Info().Msg("Context cancelled, stopping job producer...")
            close(jobsChannel) // Signal workers to stop
            dequeueJobs(jobsChannel, constants.OpenJobStatus, insertJob) // Persist remaining
            return

        case <-ticker.C: // Periodic job fetching
            // BACKPRESSURE: Skip if queue is too full
            if len(jobsChannel) >= inQueSize {
                continue
            }

            jobs, err := insertJob.JobsRepository.ListByFilters(models.JobsFilters{
                JobType: constants.InsertFileJobType,
                Status:  []string{constants.OpenJobStatus},
                Limit:   1,
            })

            if err != nil {
                log.Error().Err(err).Msg("Failed to list jobs")
                continue
            }

            // Mark as IN_QUEUE before pushing to channel
            for _, job := range jobs {
                job.Status = constants.InQueueJobStatus
            }
            if err = insertJob.JobsRepository.BulkUpsert(jobs); err != nil {
                continue
            }

            // Distribute to workers
            for _, job := range jobs {
                jobsChannel <- *job
                log.Info().Msgf("Job pushed to channel: %s", job.UUID)
            }
        }
    }
}
```

**Key Patterns Demonstrated:**
| Pattern | Implementation | Purpose |
|---------|---------------|---------|
| **Buffered Channel** | `make(chan ModelJobs, 1000)` | Prevents producer blocking |
| **Backpressure** | `len(jobsChannel) >= inQueSize` | Prevents memory exhaustion |
| **Select Statement** | `select { case <-ctx.Done(): ... }` | Non-blocking multiplexing |
| **Graceful Shutdown** | `close(jobsChannel)` + `dequeueJobs()` | No job loss on termination |
| **Ticker-based Polling** | `time.NewTicker()` | Controlled database polling |

---

### 3. Signal Handling with Context Cancellation

**Problem:** Kubernetes/Docker sends SIGTERM, need to cleanly shut down without losing data.

**Solution:** Context propagation pattern:

```go
// root.go - Enterprise-grade graceful shutdown
func Execute() {
    ctx, cancel := context.WithCancel(context.Background())
    defer cancel()

    sigs := make(chan os.Signal, 1)
    signal.Notify(sigs, syscall.SIGTERM, syscall.SIGINT)
    
    go func() {
        <-sigs     // Block until signal received
        cancel()   // Propagate cancellation to all goroutines
    }()

    err := rootCmd.ExecuteContext(ctx) // Context passed to all commands
    if err != nil {
        os.Exit(1)
    }
}

// HTTP server graceful shutdown
func startServer() {
    srv := &http.Server{Addr: ":8000", Handler: router}

    go func() {
        log.Info().Msg("Starting server on :8000")
        if err := srv.ListenAndServe(); err != nil && err != http.ErrServerClosed {
            log.Error().Err(err).Msg("Error starting server")
        }
    }()

    quit := make(chan os.Signal, 1)
    signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
    <-quit

    log.Info().Msg("Shutting down server...")
    if err := srv.Shutdown(context.TODO()); err != nil {
        log.Error().Err(err).Msg("Server forced to shutdown")
    }
}
```

**Production Benefits:**
- âœ… Clean shutdown in Kubernetes (respects terminationGracePeriod)
- âœ… No orphaned goroutines or connection leaks
- âœ… Jobs safely dequeued and persisted before exit
- âœ… HTTP connections properly drained

---

## ğŸ”„ Advanced Concurrency Implementation

### Concurrency Primitives Matrix

| Primitive | Location | Usage Pattern | Why This Choice |
|-----------|----------|---------------|-----------------|
| `sync.WaitGroup` | Batch upserts, parallel fetches | Wait for N goroutines | Simpler than errgroup for our use case |
| `sync.Mutex` | Error aggregation, rate limiter | Protect shared state | Fastest for simple critical sections |
| `sync.Once` | Rate limiter initialization | Singleton pattern | Guarantees exactly-once execution |
| `chan T` (buffered) | Job queue (1000 capacity) | Producer-consumer | Decouples producers from consumers |
| `context.Context` | Request lifecycle, shutdown | Cancellation propagation | Go idiom for timeouts/cancellation |
| `time.Ticker` | Job polling, token refill | Periodic operations | More efficient than time.Sleep loops |
| `select` | Non-blocking operations | Channel multiplexing | Handle multiple channels elegantly |

### Parallel Data Fetching with Conditional Execution

```go
// contactService.go - Demonstrating advanced concurrent fetch pattern
func (s *ContactService) ListByFilters(query utilities.VQLQuery) ([]helper.ContactResponse, error) {
    // Phase 1: Get IDs from Elasticsearch (fast search)
    sourcefields := []string{"id", "company_id"}
    elasticQuery := query.ToElasticsearchQuery(false, sourcefields)
    elasticContacts, err := s.contactElasticRepository.ListByQueryMap(elasticQuery)
    if err != nil {
        return nil, err
    }

    // Extract IDs for PostgreSQL lookup
    contactUuids, companyIds := make([]string, 0), make([]string, 0)
    for _, contact := range elasticContacts {
        contactUuids = append(contactUuids, contact.Id)
        companyIds = append(companyIds, contact.CompanyID)
    }

    // Phase 2: Parallel fetch from PostgreSQL
    var (
        pgContacts []*models.PgContact
        companies  []*models.PgCompany
        contactErr error
        companyErr error
    )
    var wg sync.WaitGroup
    
    // Always fetch contacts
    wg.Add(1)
    go func() {
        defer wg.Done()
        pgContacts, contactErr = s.contactPgRepository.ListByFilters(models.PgContactFilters{
            Uuids:         utilities.UniqueStringSlice(contactUuids),
            SelectColumns: query.SelectColumns,
        })
    }()

    // CONDITIONAL: Only fetch companies if populate=true (optimization)
    shouldPopulateCompanies := query.CompanyConfig != nil && query.CompanyConfig.Populate
    if shouldPopulateCompanies {
        wg.Add(1)
        go func() {
            defer wg.Done()
            companies, companyErr = s.companyPgRepository.ListByFilters(models.PgCompanyFilters{
                Uuids:         utilities.UniqueStringSlice(companyIds),
                SelectColumns: query.CompanyConfig.SelectColumns,
            })
        }()
    }
    
    wg.Wait() // Wait for all fetches

    if contactErr != nil || companyErr != nil {
        return nil, constants.FailedToFetchDataError
    }

    // Phase 3: In-memory join (O(n) with hash map)
    contactResponses := make([]helper.ContactResponse, 0, len(pgContacts))
    for _, contact := range pgContacts {
        contactResponses = append(contactResponses, helper.ContactResponse{
            PgContact: contact,
            Company:   nil,
        })
    }

    if shouldPopulateCompanies {
        companiesMap := make(map[string]*models.PgCompany)
        for _, company := range companies {
            companiesMap[company.UUID] = company
        }
        for i := range contactResponses {
            contactResponses[i].Company = companiesMap[contactResponses[i].PgContact.CompanyID]
        }
    }

    return contactResponses, nil
}
```

**Optimization Insights:**
- **Conditional parallelism**: Only fetch related data when requested
- **Hash map join**: O(n) instead of O(nÂ²) nested loop
- **UniqueStringSlice**: Deduplicate before query to reduce DB load
- **Field projection**: Only select requested columns

---

## ğŸ” Custom Query Language (VQL) - Domain-Specific Language Design

### VQL (Vivek Query Language) - A DSL for Search

VQL abstracts complex Elasticsearch DSL into a clean, frontend-friendly JSON interface:

### Query Structure

```json
{
  "where": {
    "text_matches": {
      "must": [
        {
          "filter_key": "title",
          "text_value": "Software Engineer",
          "search_type": "shuffle",
          "fuzzy": true,
          "operator": "and"
        }
      ],
      "must_not": [
        {
          "filter_key": "title",
          "text_value": "Intern",
          "search_type": "exact"
        }
      ]
    },
    "keyword_match": {
      "must": {
        "seniority": ["senior", "lead", "principal"],
        "country": "united states"
      },
      "must_not": {
        "email_status": "invalid"
      }
    },
    "range_query": {
      "must": {
        "company_employees_count": { "gte": 100, "lte": 5000 },
        "company_annual_revenue": { "gte": 1000000 }
      }
    }
  },
  "order_by": [
    { "order_by": "created_at", "order_direction": "desc" }
  ],
  "page": 1,
  "limit": 25,
  "select_columns": ["first_name", "last_name", "email", "title"],
  "company_config": {
    "populate": true,
    "select_columns": ["name", "website", "industries"]
  }
}
```

### Search Types & Elasticsearch Mapping

| VQL Search Type | Elasticsearch Query | Use Case | Example |
|-----------------|-------------------|----------|---------|
| `exact` | `match_phrase` with slop | Exact phrase matching | "Senior Engineer" finds "Senior Software Engineer" |
| `shuffle` | `match` with operator | Multi-word, order doesn't matter | "Engineer Senior" matches "Senior Engineer" |
| `substring` | N-gram `.ngram` field | Partial text matching | "Eng" â†’ "Engineer", "Engineering" |

### VQL to Elasticsearch Translation Engine

```go
// query.go - The VQL compiler
func (q *VQLQuery) ToElasticsearchQuery(forCount bool, sourceFields []string) map[string]any {
    resultQuery := make(map[string]any)
    
    if !forCount {
        resultQuery["_source"] = sourceFields  // Field projection (optimization)
        q.addPagination(resultQuery)            // Offset or search_after
        q.addSort(resultQuery)                  // Multi-field sorting
    }

    boolQuery := q.buildBoolQuery()
    
    // Empty query optimization
    if q.isEmpty() || len(boolQuery) == 0 {
        resultQuery["query"] = map[string]any{"match_all": map[string]any{}}
        return resultQuery
    }
    
    resultQuery["query"] = map[string]any{"bool": boolQuery}
    return resultQuery
}

func buildTextQueries(conditions []TextMatchStruct, isMust bool) []map[string]any {
    result := make([]map[string]any, 0)
    queryMap := make(map[string][]map[string]any)
    
    for _, condition := range conditions {
        switch condition.SearchType {
        case constants.SearchTypeExact:
            // match_phrase for exact order, slop for word gaps
            queryMap[condition.FilterKey] = append(queryMap[condition.FilterKey], map[string]any{
                "match_phrase": map[string]any{
                    condition.FilterKey: map[string]any{
                        "query": condition.TextValue,
                        "slop":  condition.Slop, // Allow N word gaps
                    },
                },
            })
            
        case constants.SearchTypeShuffle:
            // match for word-order independent, with optional fuzzy
            queryMap[condition.FilterKey] = append(queryMap[condition.FilterKey], map[string]any{
                "match": map[string]any{
                    condition.FilterKey: map[string]any{
                        "query":     condition.TextValue,
                        "operator":  InlineIf(condition.Operator != "", condition.Operator, "and"),
                        "fuzziness": InlineIf(condition.Fuzzy, "AUTO", 0),
                    },
                },
            })
            
        case constants.SearchTypeSubstring:
            // N-gram field for partial matching
            queryMap[condition.FilterKey] = append(queryMap[condition.FilterKey], map[string]any{
                "match": map[string]any{
                    condition.FilterKey + ".ngram": map[string]any{
                        "query":    condition.TextValue,
                        "operator": InlineIf(condition.Operator != "", condition.Operator, "and"),
                    },
                },
            })
        }
    }
    
    // Group queries for same field with should (OR within field)
    for _, queries := range queryMap {
        if len(queries) > 0 {
            if isMust {
                result = append(result, map[string]any{
                    "bool": map[string]any{
                        "should":               queries,
                        "minimum_should_match": 1,
                    },
                })
            } else {
                result = append(result, queries...)
            }
        }
    }
    return result
}
```

### Elasticsearch Index Design (N-gram for Substring Search)

```json
{
  "settings": {
    "number_of_shards": 6,
    "number_of_replicas": 1,
    "index": {
      "codec": "best_compression",
      "max_ngram_diff": 8,
      "analysis": {
        "analyzer": {
          "ngram_analyzer": {
            "tokenizer": "standard",
            "filter": ["lowercase", "ngram_filter"]
          }
        },
        "filter": {
          "ngram_filter": {
            "type": "ngram",
            "min_gram": 3,
            "max_gram": 10
          }
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "standard",
        "fields": {
          "ngram": { "type": "text", "analyzer": "ngram_analyzer" }
        }
      }
    }
  }
}
```

---

## ğŸ”— Hybrid Database Architecture - The CQRS-Inspired Pattern

### Why Hybrid? Trade-off Analysis

| Concern | Elasticsearch | PostgreSQL | Hybrid Solution |
|---------|--------------|------------|-----------------|
| **Full-text search** | âœ… Optimized, O(log n) | âŒ LIKE is O(n) | Use ES for search |
| **Fuzzy matching** | âœ… Built-in (Levenshtein) | âŒ Not available | Use ES |
| **ACID transactions** | âŒ Eventually consistent | âœ… Strong consistency | PostgreSQL for source of truth |
| **Complex joins** | âŒ Limited (denormalize) | âœ… Native SQL joins | PostgreSQL for complex queries |
| **Field selection** | âš ï¸ Returns full docs by default | âœ… Efficient projection | PostgreSQL for final data |
| **Range queries** | âœ… Excellent | âœ… With indexes | Either works |

### Two-Phase Query Execution Flow

```
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚     VQL QUERY       â”‚
                            â”‚  (from Frontend)    â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                         PHASE 1: SEARCH (Elasticsearch)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                       â”‚
                                       â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      VQL â†’ ES Query Builder      â”‚
                    â”‚                                  â”‚
                    â”‚  {                               â”‚
                    â”‚    "query": { "bool": {...} },   â”‚
                    â”‚    "_source": ["id"],            â”‚  â—„â”€â”€ Only fetch IDs!
                    â”‚    "size": 25                    â”‚
                    â”‚  }                               â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                          [ uuid1, uuid2, uuid3, ... ]
                                       â”‚
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    PHASE 2: FETCH (PostgreSQL - Parallel)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                       â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                     â”‚
                    â–¼                                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    GOROUTINE 1      â”‚             â”‚    GOROUTINE 2      â”‚
        â”‚                     â”‚             â”‚   (if populate=true)â”‚
        â”‚ SELECT * FROM       â”‚             â”‚ SELECT * FROM       â”‚
        â”‚ contacts            â”‚             â”‚ companies           â”‚
        â”‚ WHERE uuid IN (...) â”‚             â”‚ WHERE uuid IN (...) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                                   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  sync.WaitGroup.Wait()â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     IN-MEMORY HASH JOIN      â”‚
                    â”‚                              â”‚
                    â”‚  map[companyUUID] â†’ Company  â”‚
                    â”‚  contact.Company = map[id]   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   ENRICHED RESPONSE   â”‚
                       â”‚  []ContactResponse    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why This Works Well:**
1. **Elasticsearch is fast for search** - optimized inverted index
2. **PostgreSQL is authoritative** - ACID guarantees for source of truth
3. **Minimal ES payload** - only fetch IDs, reduces network I/O
4. **Parallel fetch** - company data fetched concurrently
5. **In-memory join** - O(n) with hash map, no complex SQL JOIN

---

## ğŸ“Š Distributed Job Processing Engine

### Job State Machine

```
                                   JOB STATE MACHINE
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                    poll from DB              push to channel
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  OPEN  â”‚                â”‚ IN_QUEUE â”‚                â”‚ PROCESSING â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
             â–²                                                      â”‚
             â”‚                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                                              â”‚               â”‚
             â”‚                                         success           error
             â”‚                                              â”‚               â”‚
             â”‚                                              â–¼               â–¼
             â”‚                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                                        â”‚ COMPLETED â”‚   â”‚  FAILED  â”‚
             â”‚                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â”‚                                              âœ“              â”‚
             â”‚                                                             â”‚
             â”‚                                          if retry_count > 0 â”‚
             â”‚                                                             â–¼
             â”‚          re-enqueue                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚ RETRY_IN_QUEUE  â”‚
                                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    âœ“ Persistence: Jobs table in PostgreSQL (ACID)
    âœ“ Distribution: Buffered Go channel (1000 capacity)
    âœ“ Workers: Configurable pool (default 4)
    âœ“ Retry: Separate retry job runner with configurable count
```

### Memory-Efficient Streaming CSV Processing

**Problem:** Process multi-GB CSV files from S3 without loading into memory.

**Solution:** Streaming reader with batch processing:

```go
// s3_file_insertions.go - Memory-efficient file processing
func (i *InsertJobStruct) processCSV(reader io.Reader) error {
    csvReader := csv.NewReader(reader) // Streaming reader, not buffered!

    headers, err := csvReader.Read()
    if err != nil {
        return err
    }
    
    batchSize := conf.JobConfig.BatchSize  // Configurable (default 500)
    batch := make([]map[string]string, 0, batchSize)

    for {
        row, err := csvReader.Read()
        if errors.Is(err, io.EOF) {
            break
        }
        if err != nil {
            return err
        }

        batch = append(batch, rowToMap(headers, row))
        
        // Process in batches to control memory
        if len(batch) >= batchSize {
            if err := i.BatchUpsertService.ProcessBatchUpsert(batch); err != nil {
                return err
            }
            batch = batch[:0]  // REUSE slice memory (no allocation!)
        }
    }

    // Process remaining records
    if len(batch) > 0 {
        return i.BatchUpsertService.ProcessBatchUpsert(batch)
    }
    return nil
}
```

**Memory Optimization Techniques:**
| Technique | Implementation | Benefit |
|-----------|---------------|---------|
| **Streaming from S3** | `connections.S3Connection.ReadFileStream()` | Never loads full file |
| **Batch processing** | `batchSize` chunks | Bounded memory usage |
| **Slice reuse** | `batch = batch[:0]` | Zero allocations per batch |
| **Defer close** | `defer fileStream.Close()` | Proper resource cleanup |

---

## ğŸ” Security & Reliability Patterns

### Token Bucket Rate Limiter (From Scratch)

**Algorithm:** Token bucket with configurable rate and capacity.

```go
// rateMiddleware.go - Thread-safe token bucket implementation
var (
    tokens      int
    maxLimit    int
    fillingRate int
    mu          sync.Mutex
    once        sync.Once
)

// Background token refiller (runs every second)
func startTokenRefiller() {
    go func() {
        ticker := time.NewTicker(time.Second)
        defer ticker.Stop()
        for range ticker.C {
            mu.Lock()
            if tokens < maxLimit {
                tokens = min(maxLimit, tokens+fillingRate)  // Cap at max
            }
            mu.Unlock()
        }
    }()
}

func RateLimiter() gin.HandlerFunc {
    // sync.Once ensures initialization happens exactly once
    once.Do(func() {
        maxLimit = conf.AppConfig.MaxRequestsPerMinute
        tokens, fillingRate = maxLimit, maxLimit/60  // Per-second refill
        startTokenRefiller()
    })

    return func(c *gin.Context) {
        mu.Lock()
        if tokens <= 0 {
            mu.Unlock()
            c.AbortWithStatusJSON(http.StatusTooManyRequests, gin.H{
                "error":   "rate limit exceeded",
                "message": "too many requests, please try again later",
            })
            return
        }
        tokens--
        mu.Unlock()
        c.Next()
    }
}
```

**Engineering Highlights:**
- **`sync.Once`**: Guarantees exactly-once initialization (singleton pattern)
- **`sync.Mutex`**: Protects token count from race conditions
- **Per-second refill**: Smoother than per-minute burst
- **Configurable**: Rate set via environment variable

### API Key Authentication

```go
// authMiddleware.go - Simple but effective API key auth
func APIKeyAuth() gin.HandlerFunc {
    return func(c *gin.Context) {
        apiKey := c.GetHeader("X-API-Key")
        if apiKey == "" || apiKey != conf.AppConfig.APIKey {
            c.AbortWithStatusJSON(http.StatusUnauthorized, gin.H{
                "error":   "unauthorized",
                "message": "invalid API key",
            })
            return
        }
        c.Next()
    }
}
```

---

## âš¡ Performance Engineering

### Connection Pooling Configuration

```go
// pgsql.go - PostgreSQL connection pool tuning
func (c *PgsqlConnection) Open() {
    // ... connection setup ...
    
    sqldb.SetMaxOpenConns(40)         // Max concurrent connections
    sqldb.SetMaxIdleConns(20)         // Connections kept in pool
    sqldb.SetConnMaxLifetime(30 * time.Minute)  // Prevent stale connections
    sqldb.SetConnMaxIdleTime(30 * time.Minute)  // Idle connection timeout
}

// elastic_search.go - Elasticsearch HTTP transport tuning
cfg := elasticsearch.Config{
    Transport: &http.Transport{
        MaxIdleConns:        20,      // Global idle connections
        MaxIdleConnsPerHost: 5,       // Per-host idle connections
        IdleConnTimeout:     30 * time.Minute,
        DisableCompression:  false,   // Enable compression for search
        DialContext: (&net.Dialer{
            Timeout:   30 * time.Second,   // Connection timeout
            KeepAlive: 30 * time.Second,   // TCP keepalive
        }).DialContext,
    },
    CompressRequestBody: !c.Config.Debug,  // Compress in production
}
```

### Bulk Operations for Database Efficiency

```go
// contact.pgsql.repo.go - Efficient bulk upsert with conflict resolution
func (t *PgContactStruct) BulkUpsert(contacts []*PgContact) (int64, error) {
    _, err := t.PgDbClient.NewInsert().
        Model(&contacts).
        On("CONFLICT(uuid) DO UPDATE").  // Atomic upsert
        Set("first_name = EXCLUDED.first_name").
        Set("last_name = EXCLUDED.last_name").
        // ... all fields ...
        Set("updated_at = EXCLUDED.updated_at").
        Exec(context.Background())
    
    return int64(len(contacts)), err
}

// contact.elastic.repo.go - Elasticsearch bulk API
func (t *ElasticContactStruct) BulkUpsert(contacts []*ElasticContact) (int64, error) {
    var buf bytes.Buffer
    for _, contact := range contacts {
        meta := map[string]any{
            "index": map[string]any{
                "_index": constants.ContactIndex,
                "_id":    contact.Id,  // Upsert by ID
            },
        }
        utilities.AddToBuffer(&buf, meta)
        utilities.AddToBuffer(&buf, contact)
    }

    response, err := t.ElasticClient.Bulk(bytes.NewReader(buf.Bytes()))
    // ... error handling ...
    return int64(len(contacts)), nil
}
```

### Deterministic UUID Generation (Idempotency)

```go
// common.go - SHA1-based UUID5 for idempotent operations
func GenerateUUID5(value string) string {
    return uuid.NewSHA1(uuid.NameSpaceURL, []byte(value)).String()
}

// Usage in contact creation
ContactUUID := utilities.GenerateUUID5(fmt.Sprintf("%s%s%s", FirstName, LastName, LinkedinURL))
```

**Why This Matters:**
- **Idempotent imports**: Re-running same data won't create duplicates
- **Deterministic keys**: Same input always produces same UUID
- **Deduplication**: Natural prevention of duplicate records

---

## ğŸ¨ Design Patterns & SOLID Principles

### Pattern Catalog

| Pattern | Implementation | SOLID Principle |
|---------|---------------|-----------------|
| **Repository Pattern** | Separate repos per data store | Single Responsibility |
| **Factory Functions** | `NewContactService()`, `NewBatchUpsertService()` | Dependency Inversion |
| **Interface Segregation** | Small, focused repo interfaces | Interface Segregation |
| **Dependency Injection** | Services receive interfaces | Dependency Inversion |
| **Builder Pattern** | `filters.ToQuery(queryBuilder)` | Single Responsibility |
| **Worker Pool** | Configurable goroutine pool | Open/Closed |
| **Singleton** | `sync.Once` for rate limiter | - |
| **Strategy Pattern** | Search types (exact, shuffle, substring) | Open/Closed |

### Clean Architecture Layers

```
                           CLEAN ARCHITECTURE
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚            CONTROLLERS (HTTP)               â”‚
           â”‚                                             â”‚
           â”‚   â€¢ Request binding & validation            â”‚
           â”‚   â€¢ Response formatting                     â”‚
           â”‚   â€¢ HTTP status codes                       â”‚
           â”‚   â€¢ NO business logic                       â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚            SERVICES (BUSINESS)              â”‚
           â”‚                                             â”‚
           â”‚   â€¢ Orchestrates data flow                  â”‚
           â”‚   â€¢ Implements business rules               â”‚
           â”‚   â€¢ Manages concurrency                     â”‚
           â”‚   â€¢ Data transformation                     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚           REPOSITORIES (DATA)               â”‚
           â”‚                                             â”‚
           â”‚   â€¢ Database queries                        â”‚
           â”‚   â€¢ Elasticsearch operations                â”‚
           â”‚   â€¢ Interface-based (testable)              â”‚
           â”‚   â€¢ NO business logic                       â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Interface-Based Repository Design

```go
// models/contact.pgsql.repo.go - Interface segregation
type PgContactSvcRepo interface {
    GetFiltersByQuery(query FiltersDataQuery) ([]*PgContact, error)
    ListByFilters(filters PgContactFilters) ([]*PgContact, error)
    BulkUpsert(contacts []*PgContact) (int64, error)
}

// Concrete implementation
type PgContactStruct struct {
    PgDbClient *bun.DB
}

// Factory function for dependency injection
func PgContactRepository(db *bun.DB) PgContactSvcRepo {
    return &PgContactStruct{PgDbClient: db}
}

// Service uses interface, not concrete type (testable, mockable)
type ContactService struct {
    contactPgRepository models.PgContactSvcRepo  // Interface type!
    // ...
}
```

---

## ğŸ“ˆ Scalability Considerations

### Horizontal Scaling Strategy

```
                              HORIZONTAL SCALING ARCHITECTURE
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚    LOAD BALANCER    â”‚
                              â”‚    (nginx / ALB)    â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                     â”‚                     â”‚
                   â–¼                     â–¼                     â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   API-1    â”‚        â”‚   API-2    â”‚        â”‚   API-N    â”‚
            â”‚   :8000    â”‚        â”‚   :8000    â”‚        â”‚   :8000    â”‚
            â”‚ (Stateless)â”‚        â”‚ (Stateless)â”‚        â”‚ (Stateless)â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   JOB-1    â”‚        â”‚   JOB-2    â”‚        â”‚   JOB-N    â”‚
            â”‚  worker=4  â”‚        â”‚  worker=4  â”‚        â”‚  worker=4  â”‚
            â”‚ (Stateless)â”‚        â”‚ (Stateless)â”‚        â”‚ (Stateless)â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                     â”‚                     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                        Jobs pulled from DB with status lock
                        (IN_QUEUE prevents double-processing)

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                   DATA STORES
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    POSTGRESQL    â”‚  â”‚  ELASTICSEARCH   â”‚  â”‚      AWS S3      â”‚
        â”‚   (Primary +     â”‚  â”‚   (3-node        â”‚  â”‚    (Managed)     â”‚
        â”‚    Replicas)     â”‚  â”‚    cluster)      â”‚  â”‚                  â”‚
        â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚
        â”‚ â€¢ Read replicas  â”‚  â”‚ â€¢ 6 shards       â”‚  â”‚ â€¢ Infinite scale â”‚
        â”‚ â€¢ Pool: 40 conn  â”‚  â”‚ â€¢ 1 replica each â”‚  â”‚ â€¢ Presigned URLs â”‚
        â”‚ â€¢ ACID           â”‚  â”‚ â€¢ Horizontal     â”‚  â”‚ â€¢ Streaming      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Performance Characteristics

| Metric | Value | Configuration |
|--------|-------|---------------|
| **Concurrent DB Connections** | 40 max, 20 idle | `SetMaxOpenConns(40)` |
| **ES Connection Pool** | 20 idle, 5 per host | Custom HTTP transport |
| **Job Queue Capacity** | 1000 jobs | Buffered channel |
| **Parallel Workers** | Configurable (default 4) | `PARALLEL_JOBS` env |
| **Batch Size** | Configurable (default 500) | `BATCH_SIZE_FOR_INSERTION` |
| **Rate Limit** | Configurable req/min | Token bucket algorithm |
| **ES Shards** | 6 primary, 1 replica | Index settings |

---

## ğŸ›¡ Error Handling & Resilience

### Centralized Error Definitions

```go
// constants/errors_message.go - Typed errors for consistent handling
var (
    InvalidCredentialsError     = errors.New("invalid credentials")
    UserAccountDeletedError     = errors.New("user account is deleted")
    FailedToHashPasswordError   = errors.New("failed to hash password")
    
    FailedToGenerateTokenError  = errors.New("failed to generate token")
    InvalidOrExpiredTokenError  = errors.New("invalid or expired token")
    
    PageSizeExceededError       = errors.New("page size exceeds maximum limit")
    PageNumberExceededError     = errors.New("page number exceeds maximum limit")
    FailedToFetchDataError      = errors.New("failed to fetch data")
)
```

### Validation Layer

```go
// utilities/common.go - Pagination validation
func ValidatePageSize(limit int) error {
    if limit > constants.MaxPageSize {
        return constants.PageSizeExceededError
    }
    return nil
}

func ValidateElasticPagination(page, limit int) error {
    if limit > constants.MaxPageSize {
        return constants.PageSizeExceededError
    }
    if page > constants.MaxElasticPageNumber {
        return constants.PageNumberExceededError
    }
    return nil
}
```

### Job Retry Mechanism

```go
// Separate retry worker for failed jobs
func RetryInsertFileJob(ctx context.Context) {
    insertJob := NewInsertJob()
    jobsChannel := make(chan models.ModelJobs, 1000)
    
    wg.Add(1)
    go insertJob.Run(&wg, 1, jobsChannel)  // retry=1 decrements retry_count

    ticker := time.NewTicker(time.Duration(conf.JobConfig.TickerInterval) * time.Minute)
    
    for {
        select {
        case <-ctx.Done():
            close(jobsChannel)
            dequeueJobs(jobsChannel, constants.FailedJobStatus, insertJob)
            return
        case <-ticker.C:
            jobs, _ := insertJob.JobsRepository.ListByFilters(models.JobsFilters{
                JobType:  constants.InsertFileJobType,
                Retrying: true,  // Only jobs with retry_count > 0
                Status:   []string{constants.FailedJobStatus},
                Limit:    1,
            })
            // ... process retry jobs ...
        }
    }
}
```

---

## ğŸ“¡ API Reference

### Contacts API

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/contacts/` | Query contacts with VQL |
| `POST` | `/contacts/count` | Get count of matching contacts |
| `POST` | `/contacts/batch-upsert` | Bulk upsert contacts |

### Companies API

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/companies/` | Query companies with VQL |
| `POST` | `/companies/count` | Get count of matching companies |
| `POST` | `/companies/batch-upsert` | Bulk upsert companies |

### Common API

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/common/:service/filters` | Get available filters for a service |
| `POST` | `/common/:service/filters/data` | Get filter options/values |
| `GET` | `/common/upload-url?filename=X` | Generate S3 presigned upload URL |
| `POST` | `/common/batch-upsert` | Batch upsert from raw CSV-like data |
| `POST` | `/common/jobs` | List jobs with filters |
| `POST` | `/common/jobs/create` | Create a new background job |

### Health Check

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/health` | Health check endpoint |

---

## ğŸ“ Project Structure

```
connectra/
â”œâ”€â”€ cmd/                              # CLI commands (Cobra)
â”‚   â”œâ”€â”€ root.go                       # Root command with graceful shutdown
â”‚   â”œâ”€â”€ server.go                     # API server command
â”‚   â””â”€â”€ s3_file_insertions.go         # Background job runner
â”‚
â”œâ”€â”€ conf/                             # Configuration management
â”‚   â””â”€â”€ viper.go                      # Viper-based env config with reflection
â”‚
â”œâ”€â”€ connections/                      # Database & service connections (singletons)
â”‚   â”œâ”€â”€ database.go                   # PostgreSQL connection pool
â”‚   â”œâ”€â”€ s3_connection.go              # AWS S3 client
â”‚   â””â”€â”€ search_engine.go              # Elasticsearch client
â”‚
â”œâ”€â”€ clients/                          # Low-level client implementations
â”‚   â”œâ”€â”€ pgsql.go                      # PostgreSQL with Bun ORM, connection pooling
â”‚   â”œâ”€â”€ elastic_search.go             # Elasticsearch with custom HTTP transport
â”‚   â”œâ”€â”€ s3.go                         # AWS S3 with presigned URLs, streaming
â”‚   â””â”€â”€ mongo.go                      # MongoDB client (extensible)
â”‚
â”œâ”€â”€ middleware/                       # HTTP middlewares
â”‚   â”œâ”€â”€ authMiddleware.go             # API key authentication
â”‚   â””â”€â”€ rateMiddleware.go             # Token bucket rate limiter
â”‚
â”œâ”€â”€ models/                           # Data models & repositories
â”‚   â”œâ”€â”€ contact.pgsql.go              # PostgreSQL contact model
â”‚   â”œâ”€â”€ contact.pgsql.repo.go         # Contact repository (interface + impl)
â”‚   â”œâ”€â”€ contact.elastic.go            # Elasticsearch contact model
â”‚   â”œâ”€â”€ contact.elastic.repo.go       # Elasticsearch contact repository
â”‚   â”œâ”€â”€ company.pgsql.go              # PostgreSQL company model
â”‚   â”œâ”€â”€ company.pgsql.repo.go         # Company repository
â”‚   â”œâ”€â”€ company.elastic.go            # Elasticsearch company model
â”‚   â”œâ”€â”€ company.elastic.repo.go       # Elasticsearch company repository
â”‚   â”œâ”€â”€ jobs.go                       # Job model with state machine
â”‚   â”œâ”€â”€ jobs.repo.go                  # Job repository with deduplication
â”‚   â”œâ”€â”€ filters.go                    # Filter configuration model
â”‚   â”œâ”€â”€ filters.repository.go         # Filters repository
â”‚   â”œâ”€â”€ filters_data.go               # Filter data model
â”‚   â””â”€â”€ filters_data.repository.go    # Filter data repository
â”‚
â”œâ”€â”€ modules/                          # Feature modules (Clean Architecture)
â”‚   â”œâ”€â”€ contacts/
â”‚   â”‚   â”œâ”€â”€ controller/
â”‚   â”‚   â”‚   â””â”€â”€ contactController.go  # HTTP handlers
â”‚   â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â”‚   â””â”€â”€ contactService.go     # Business logic with concurrency
â”‚   â”‚   â”œâ”€â”€ helper/
â”‚   â”‚   â”‚   â”œâ”€â”€ requests.go           # Request DTOs & validation
â”‚   â”‚   â”‚   â””â”€â”€ responses.go          # Response DTOs
â”‚   â”‚   â””â”€â”€ routes.go                 # Route registration
â”‚   â”‚
â”‚   â”œâ”€â”€ companies/
â”‚   â”‚   â”œâ”€â”€ controller/
â”‚   â”‚   â”‚   â””â”€â”€ companyController.go
â”‚   â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â”‚   â””â”€â”€ companyService.go
â”‚   â”‚   â”œâ”€â”€ helper/
â”‚   â”‚   â”‚   â”œâ”€â”€ requests.go
â”‚   â”‚   â”‚   â””â”€â”€ response.go
â”‚   â”‚   â””â”€â”€ routes.go
â”‚   â”‚
â”‚   â””â”€â”€ common/
â”‚       â”œâ”€â”€ controller/
â”‚       â”‚   â”œâ”€â”€ batchInsertController.go
â”‚       â”‚   â”œâ”€â”€ filterController.go
â”‚       â”‚   â”œâ”€â”€ jobController.go
â”‚       â”‚   â””â”€â”€ uploadController.go
â”‚       â”œâ”€â”€ service/
â”‚       â”‚   â”œâ”€â”€ batchInsertService.go  # Parallel writes to 5 stores
â”‚       â”‚   â”œâ”€â”€ filterService.go
â”‚       â”‚   â””â”€â”€ jobService.go
â”‚       â”œâ”€â”€ helper/
â”‚       â”‚   â”œâ”€â”€ requests.go
â”‚       â”‚   â””â”€â”€ responses.go
â”‚       â””â”€â”€ routes.go
â”‚
â”œâ”€â”€ jobs/                             # Background job workers
â”‚   â”œâ”€â”€ s3_file_insertions.go         # Worker pool with channel-based queue
â”‚   â””â”€â”€ insert_direct_file.go         # Direct file insertion
â”‚
â”œâ”€â”€ utilities/                        # Shared utilities
â”‚   â”œâ”€â”€ query.go                      # VQL to Elasticsearch converter
â”‚   â”œâ”€â”€ structures.go                 # VQL type definitions
â”‚   â””â”€â”€ common.go                     # Helper functions (UUID5, reflection)
â”‚
â”œâ”€â”€ constants/                        # Application constants
â”‚   â”œâ”€â”€ elastic_serach.go             # ES index names, search types
â”‚   â”œâ”€â”€ errors_message.go             # Typed error definitions
â”‚   â”œâ”€â”€ jobs.go                       # Job states and types
â”‚   â””â”€â”€ services.go                   # Service identifiers
â”‚
â”œâ”€â”€ examples/                         # Example configurations
â”‚   â”œâ”€â”€ company_index_create.json     # Elasticsearch company index mapping
â”‚   â”œâ”€â”€ contact_index_create.json     # Elasticsearch contact index mapping
â”‚   â””â”€â”€ nql_query_input.json          # Example VQL query
â”‚
â”œâ”€â”€ Dockerfile                        # Multi-stage build (alpine)
â”œâ”€â”€ go.mod                            # Go modules
â”œâ”€â”€ go.sum                            # Dependency checksums
â””â”€â”€ main.go                           # Application entry point
```

---

## ğŸš€ Getting Started

### Prerequisites

- Go 1.24+
- PostgreSQL 15+
- Elasticsearch 8.x
- AWS S3 (or MinIO for local development)
- Docker (optional)

### Environment Variables

```env
# Application
APP_ENV=development
API_KEY=your-secret-api-key
MAX_REQUESTS_PER_MINUTE=1000

# PostgreSQL
PG_DB_HOST=localhost
PG_DB_PORT=5432
PG_DB_DATABASE=connectra
PG_DB_USERNAME=postgres
PG_DB_PASSWORD=password
PG_DB_SSL=false
PG_DB_DEBUG=true

# Elasticsearch
ELASTICSEARCH_HOST=localhost
ELASTICSEARCH_PORT=9200
ELASTICSEARCH_USERNAME=elastic
ELASTICSEARCH_PASSWORD=password
ELASTICSEARCH_SSL=false
ELASTICSEARCH_AUTH=true
ELASTICSEARCH_DEBUG=false

# AWS S3
S3_ACCESS_KEY=your-access-key
S3_SECRET_KEY=your-secret-key
S3_REGION=us-east-1
S3_BUCKET=connectra-uploads
S3_ENDPOINT=s3.amazonaws.com
S3_SSL=true
S3_UPLOAD_FILE_PATH_PREFIX=uploads

# Jobs Configuration
PARALLEL_JOBS=4
BATCH_SIZE_FOR_INSERTION=500
TICKER_INTERVAL_MINUTES=5
JOB_IN_QUEUE_SIZE=100
JOB_TYPE=normal
```

### Running Locally

```bash
# Clone the repository
git clone https://github.com/yourusername/connectra.git
cd connectra

# Install dependencies
go mod download

# Create .env file
cp .env.example .env

# Run the API server
go run main.go api-server

# Or run the background job processor
go run main.go s3-job
```

### Docker Deployment

```bash
# Build the image
docker build -t connectra:latest .

# Run API server
docker run -d \
  --env-file .env \
  -e RUN_COMMAND=api-server \
  -p 8000:8000 \
  --name connectra-api \
  connectra:latest

# Run job processor
docker run -d \
  --env-file .env \
  -e RUN_COMMAND=s3-job \
  --name connectra-jobs \
  connectra:latest
```

### Creating Elasticsearch Indices

```bash
# Create contacts index
curl -X PUT "localhost:9200/contacts_index" \
  -H "Content-Type: application/json" \
  -d @examples/contact_index_create.json

# Create companies index
curl -X PUT "localhost:9200/companies_index" \
  -H "Content-Type: application/json" \
  -d @examples/company_index_create.json
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ“„ License

MIT License - feel free to use this project as a reference or starting point.

---

<p align="center">
  <sub>Built with â¤ï¸ in Go | Designed for scalability, built for production</sub>
</p>

<p align="center">
  <strong>Demonstrates: System Design â€¢ Concurrency â€¢ Clean Architecture â€¢ API Design â€¢ Database Optimization</strong>
</p>
